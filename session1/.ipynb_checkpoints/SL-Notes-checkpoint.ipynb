{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101 in Python\n",
    "### Led by Matteo Hessel, UCL ML Alumni, currently a Research Engineer at Google DeepMind\n",
    "\n",
    "[Video recording available here](https://youtu.be/AuMiRIYxyl8?list=PLBYyZ2a9bVqyN34JPCzEinA_cXT2ckrCa)\n",
    "\n",
    "# Preparation\n",
    "\n",
    "## Why Python?\n",
    "If you want to be a Data Scientist and apply machine learning, regardless of the application domain you will encounter Python. The reasons that make it an excellent language for data science are several:\n",
    "\n",
    "First of all is a very simple language, if you know any other programming language you will learn it very fast.\n",
    "\n",
    "Second, is very flexible, it offers features from imperative, scripting, object-oriented, and functional programming.\n",
    "This flexibility means that you can do very fast prototyping, because you can use it at the level of abstraction you need it, if you want to run some tests with different models you can do it with few lines of code without caring about structure and classes, and potentially do so when you want to put the selected model into production. \n",
    "If you try to do the same with Java you will have 20 classes, 5 interfaces before you can even start tackling your problem...\n",
    "\n",
    "Third, you can rely on countless libraries, developed open source by a huge community, you have:\n",
    "- NumPy, which wraps highly optimized C code, for fast matrix operations, \n",
    "- Pandas, which integrates naturally with Numpy, offers tools for data loading, formatting, and transformation, \n",
    "- NLTK offers tools for natural language processing (tokenizing, sentence splitting, parsing), all of these are trivial to implement but will take hours of boring coding if you have to do it yourself,\n",
    "- Scikit-learn, which we shall see during the workshop, integrated with Numpy and Pandas, offers many models for statistics and machine learning.\n",
    "- Finally Theano, Chainer, Tensor Flow offer neural network frameworks and easy integration of high performance computing hardware such as GPUs.\n",
    "\n",
    "### The fundamental features of Python\n",
    "- Interpreted language\n",
    "- Dynamic Typing\n",
    "- Two versions: Python2.x and Python3.x\n",
    "- Just in time compiler: PyPy\n",
    "- First class functions\n",
    "- Class system and Inheritance\n",
    "\n",
    "Remember to **indent**!\n",
    "\n",
    "\n",
    "## What is Jupyter?\n",
    "Jupyter is a multi-language open-source project which provides notebooks, web-based interactive scripting environments, for many uses. Particularly useful for data scientists. iPython is a python kernel for jupiter.\n",
    "\n",
    "Installing Jupyter:\n",
    "- Download [Anaconda Python 3.5](https://www.continuum.io/downloads)\n",
    "- Start the installer of Conda\n",
    "- Check installation typing `python` in Terminal (exit with Ctrl+D)\n",
    "Install / update Jupyter typing `conda install jupyter` in Terminal\n",
    "Run jupyter typing `jupyter notebook` in Terminal\n",
    "\n",
    "Install the packages:\n",
    "```\n",
    "conda install numpy\n",
    "conda install scipy\n",
    "conda install scikit-learn\n",
    "conda install pandas\n",
    "conda install nltk\n",
    "conda install matplotlib\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "Machine Learning is a collection of algorithms and techniques to create systems that can improve their performance on some task as these systems gain experience in performing it.\n",
    "\n",
    "There are 3 main area within machine learning: Supervised, Unsupervised, and Reinforcement Learning. \n",
    "These forms of learning differ on the type of experience the systems are exposed to.\n",
    "\n",
    "![The Triangle](img/thetriangle.png)\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised Learning is the simplest form of Machine Learning.\n",
    "\n",
    "In Supervised Learning the **experience** comes as a set of input and outputs pairs, these input and output pairs are presented to the system, and the system must learn to predict the output given the input.\n",
    "\n",
    "For example you may be given a set of houses in London, for which you are given size, area, floor and some other features of the house, together with the price they were sold for.\n",
    "\n",
    "One may think that this could be done just by storing and remembering the pairs.\n",
    "\n",
    "The **key issue** here is that after learning, a SL system should be able to predict correctly the output also if he is presented a new input which comes from the same setting or problem but which he has never seen before\n",
    "\n",
    "A SL system should be able to learn how to map the house features to their price, and thus estimate the price of a new house which is presented to it\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "The second big area in machine learning is Unsupervised Learning.\n",
    "In Unsupervised Learning the experience comes as a stream of **unlabelled data**. The task is to **find structure in the data**.\n",
    "\n",
    "For example, using again houses as example, you might want to find clusters of houses, or you might want to remove some of the features you are storing for each house to save space in memory or, finally you might want to find better encodings and representations for your data.\n",
    "\n",
    "Suppose you are given the length and width of the garden, maybe it’s more useful to just multiply those quantities\n",
    "and store the resulting area expressed in square meters.\n",
    "\n",
    "Overall it’s quite an heterogeneous area, but all unsupervised learning methods share the common idea\n",
    "of finding some structure in data without supervision.\n",
    "\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "In RL the experience comes as a **sequence of decision problems**.\n",
    "\n",
    "The system, usually called an agent, is **given an observation** of the environment, and is asked to **perform an action**.\n",
    "\n",
    "Actions affect the environment and therefore influences the future observation. More importantly, beside actions, the system is also provided rewards, that give an evaluative feedback.\n",
    "\n",
    "The problem is that these rewards might be time delayed. If you want to teach a system to play checkers you will get a positive reward if you win the game. But the system will have performed many different actions during the game,and must learn which were good and which were bad from these very rare signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where does ML fit into the big picture?\n",
    "\n",
    "Machine Learning is part of the field of Computer Science that investigates Artificial Intelligence.\n",
    "\n",
    "The first computers were built in the **40s**. With the first prototypes developed during WWII to crack the Nazi’s code **Enigma**. And the first general purpose computers came soon later in the US. The history of artificial intelligence research starts immediately after, in the **50s**.  \n",
    "\n",
    "**Turing** was among the first to speculate about use computers to emulate human intelligence. One of his famous papers focus on evaluation: how could we determine if a computer is indeed intelligent?\n",
    "\n",
    "He supported a very pragmatic approach, by which artificial intelligence was to be considered achieved whenever a computer was to become indistinguishable from a human being in some well chosen high level task. More specifically he identified such task with sustaining a non trivial conversation with a human being. This is now known as the Turing Test. However, nowadays, most people believe that a more broad range of abilities should be possessed by a so called artificial general intelligence. \n",
    "\n",
    "The first human capability to be addressed is planning: In **1952** when **Arthur Samuel** presented the first automatic checker player, sufficiently good to challenge a reasonably experienced human. His system was very simple but the idea that games could be a good benchmark to evaluate progress in artifical intelligence was a lasting legacies. \n",
    "\n",
    "In **1956, Herbert Simon** built the first **Automated Theorem Provers**, which aimed at investigating how computers could be provided with Logical Reasoning, one of the more distinctive human features.\n",
    "\n",
    "In **1957, Arthur Rosenblatt** focused on another fundamental human capability: learning. He developed the **perceptron**: a simple model of biological neurons, that will become a fundamental component of modern neural networks. The fundamental property of the perceptron was that it could be trained to learn to classify inputs by suitably adapting its weights based on his experience.\n",
    "\n",
    "After these initial results AI research had a sudden shock in the **70s** for several reasons. Mainly, the hype around AI had generated unrealistic expectations, given the state of knowledge at the time, and when project failed to deliver government agencies around the world cut most sources of funding paralizing research and driving industry and investors into being extremely cautious. \n",
    "\n",
    "Neural networks and AI in general started **recovering in the 80s**. For neural network research a key driver was the backpropagation algorithms. Using backpropagation more complex neural networks could be trained, and first commercial applications were developed. **Yahn LeCun** in particular was a pioneer of **Convolutional Neural Networks**.\n",
    "\n",
    "This lead artificial intelligence applications again in the spotlight, but the community started focusing on more well defined and limited tasks, rather than pursuing a true artificial general intelligence, leading to so called machine learning. Machine learning for long has focused on developing systems capable of improving their behavior based on data and experience, making less or no claims on the intelligence of the system itself.\n",
    "\n",
    "In the **90s**, machine learning focused largely on **statistical methods and probabilistic models**. But in **2006**, neural networks came back for the third time, thanks to several publications in 2006 by **Hinton, Bengio and LeCun** who after a decade of work far from the spotlight showed how we could use the recent advances on hardware, especially GPUs (Graphical processing Units) to train much bigger and more powerful networks. This led to impressive successes in a variety of fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical applications of ML\n",
    "\n",
    "Machine Learning has become the way to go in:\n",
    "- Machine Vision, \n",
    "- Natural Language Processing, \n",
    "- Robotics, \n",
    "- Web Search, and \n",
    "- Online and Mobile advertisement. \n",
    "\n",
    "Within ML, Deep Learning has mostly wiped all alternative approaches in Machine Vision, and is on the way to substitute statistical based models for NLP. \n",
    "\n",
    "Beside these domains were ML is now the only viable approach, machine Learning is also impressively growing in Marketing, Finance and Trading, the Videogame industry, Business Intelligence, Health, just to name a few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive into Supervised Learning\n",
    "\n",
    "So today we will focus on Supervised Learning.\n",
    "\n",
    "Computers do not have “experience”, what does learning *mean*? A computer system is provided experience as **data**.\n",
    "\n",
    "In Supervised Learning, **experience** comes as a **set of input and output pairs**.\n",
    "Where inputs are identified by the values of a set of attributes $ x_1, x_2, x_n. $ And outputs are some label $ y $.\n",
    "\n",
    "In the house price prediction example, **attributes** were the size, the area of london, the floor, while the **label** was the price at which the house could be sold.\n",
    "\n",
    "\n",
    "## Aim\n",
    "\n",
    "Importantly, the aim of Supervised Learning is **not** to learn by heart the set of input of output pairs.\n",
    "\n",
    "The aim is to learn something that can be **generalized** to make reliable prediction for new previously unseen samples, as long as these inputs are generated by the **same phenomenon** or problem.\n",
    "\n",
    "In other words, if new data is provided, and this data is **generated from a similar phenomenon**, problem or setting, the **system will generalize** what it has learned to such new data.\n",
    "\n",
    "\n",
    "## The two main types of SL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Classification | Regression |\n",
    "| -------------- | ---------- |\n",
    "| Labels $y$ are a **finite, discrete** set | Labels $y$ are continuous values |\n",
    "| Categories ( e.g. red, green, blue, ...) | $y \\in \\mathbb{R},  y \\in \\mathbb{R+},  y \\in [a,b], \\ldots $ |\n",
    "| ![Classification](img/classification.png) | ![Regression](img/regression.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris Dataset\n",
    "\n",
    "In order to illustrate some important concepts in classification, we will consider a small but famous dataset: the Iris dataset.\n",
    "\n",
    "This contains 150 sample flowers belonging to three types of Iris, setosa - blue - versicolor - green - and virginica - red. For each flower sample 4 attributes are listed, we shall consider 2: petal length and petal width.\n",
    "\n",
    "| Input Attributes: | Output Label: |\n",
    "| ----------------- | ------------- |\n",
    "| petal-length (PL) | setosa (BLUE)\t|\n",
    "| petal-width (PW) | versicolor (GREEN)|\n",
    "| | virginica (RED) |\n",
    "\n",
    "![Iris](img/iris.png)\n",
    "\n",
    "In the picture you can see the 150 samples plotted, the color of each sample identifies the type of iris, the location of each point corresponds to it’s petal length and width.\n",
    "\n",
    "There is some clear structure in the data we woudn’t even need to use an algorithm to create some reasonable classification rule. When however you have many variable, you cannot just make some nice plots to understand the underlying structure of data.\n",
    "\n",
    "But then you need a way to learn and express your classification reasoning algorithmically. How does “those in the bottom left corner are usually blue” translates in a precise computer understanable language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model\n",
    "\n",
    "![DecisionTreeExample](img/decisiontree.png)\n",
    "\n",
    "The first model we will consider are decision trees. Decision trees allow to **encode our reasoning as a sequence of decisions**.\n",
    "\n",
    "We have 150 samples in total, 50 of each class. The first question we may ask is the petal length smaller than 2.45?\n",
    "That immediately separates out the blue iris from all the other allowing to safely classify all samples which satisfy the condition as blue.\n",
    "\n",
    "Among those who don’t satisfy it we have 50 green and 50 red iris. As we had drawn on the picture we can now separate those with larger petal width testing if it larger than 1.75. We can then classify those who don’t fulfill this condition as RED, makling just 1 single mistake. The group of those who do satisfy this second condition is less homogeneous so we might split it again.\n",
    "\n",
    "Overall this tree encodes our reasoning in a precise way, which may be encoded in a program, and leads to very good classification accuracy: Only for 4 mistakes among 150 samples.\n",
    "\n",
    "However the decision trees are nice not only because they allow to express our rules in a form understandable by a computer, they are nice because we can then learn them from data using a **simple learning rule**.\n",
    "\n",
    "\n",
    "## Learning Decision Trees\n",
    "\n",
    "The tree is constructed RECURSIVELY.\n",
    "Start with a node containing all training examples, recursively split the node in two children containing the sample that satisfy and not satisfy some condition, respectively.\n",
    "\n",
    "The split is evaluated by trying splitting on all attributes, and choosing the split which leads to the most homogeneous children.\n",
    "\n",
    "By splitting repeatedly we can always arrive to have perfectly homogeneous leaf nodes (in the limit by having 1 example per node). In practise we have therefore to stop the splitting at a certain depth.\n",
    "\n",
    "## Evaluating a Classification Model\n",
    "\n",
    "One of you might come at me and say, well this is a very simple algorithm I have a better one. How can we decide which algorithm works best?\n",
    "\n",
    "The **aim** is that if then new data is provided, and this data is generated from a similar phenomenon, problem or setting, the system will generalize what it has learned to such new data.\n",
    "\n",
    "We need to measure how good it is at classifying samples that were not used for training the model itself (e.g. to build the decision tree) \n",
    "\n",
    "### Hold-out\n",
    "\n",
    "How can we do this?\n",
    "\n",
    "One way, the simplest is **Hold Out**: \n",
    "- split the available data in two parts (*Training Set* and *Test Set*) \n",
    "- use only instances of the *Training Set* for building the model\n",
    "- evaluate by measuring its *Accuracy* (fraction of samples of the *Test set* which are correctly classified by the model)\n",
    "\n",
    "![Hold-Out](img/holdout.png)\n",
    "\n",
    "What might be the **limitations** of this approach?\n",
    "\n",
    "A simple one is that you *waste data*, and in many cases data might be very valuable, because create the labels might be expensive.\n",
    "\n",
    "Example: I used supervised learning for automatic summarization when working on my thesis, and in that case for each document I used to train my system I needed to have multiple summaries written by a human as examples to learn from. So I didn’t want to waste the few summaries I have for evaluation. What can you do in this case?\n",
    "\n",
    "### Cross-Validation \n",
    "\n",
    "Cross validation not only gives you better **estimate of the error you can expect on future data** when you will deploy the system, but it also gives you, more importantly, a **measure of the uncertainty**, you indeed use the variability among the different folds to estimate confidence interval.\n",
    "\n",
    "Estimate error: $ \\bar{E} \\frac{\\sum_{i=1}^{k} E_i}{k} $\n",
    "\n",
    "Measure the Uncertainty:  Estimate 95% Confidence Intervals\n",
    "\n",
    "$$ \\bar{E} \\frac{\\sum_{i=1}^{k} E_i}{k} , std(E) = \\sqrt{\\frac{\\sum_{i=1}^{k}((E_i - \\bar{E})^2)}{k}} $$\n",
    "\n",
    "![Cross-Validation](img/crossvalidation.png)\n",
    "\n",
    "\n",
    "### Overfitting & Generalisation\n",
    "\n",
    "A model is overfitting when it performs very well on the data it has been trained on but fails to generalize to new data samples.\n",
    "\n",
    "![overfit](img/overfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Let's give decision trees & classification a go in the [tasks](SL-Tasks.ipynb#Iris-Flower-Classification), on the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - [Handwritten Digits with decision trees in tasks](SL-Tasks.ipynb#Handwritten-Digit-Recognition)\n",
    "\n",
    "```\n",
    "# Load low resolution Handwritten Digits\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Reshape the images as vectors\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "```\n",
    "\n",
    "#### Aims\n",
    "- Learn to classify the handwritten digits using Decision Trees\n",
    "- Evaluate accuracy using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "\n",
    "Ensembling is a powerful approach for boosting the performance of a classifier or other supervised learning model.\n",
    "\n",
    "![Ensembling](img/ensemble.png)\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Random Forests: aggregate Decision Trees, trained on random subsets of data, using in each tree a random subset of attributes.\n",
    "\n",
    "![Bagging](img/bagging.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Handwritten Digits with Random Forests in [tasks](SL-Tasks.ipynb#Train-and-Evaluate-Random-Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Example\n",
    "\n",
    "![ads vs sales](img/adsvsales.png)\n",
    "\n",
    "What are the **features**?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)\n",
    "\n",
    "### Univariate Linear Regression\n",
    "\n",
    "![Univariate LR](img/univariate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task - [Predict Sales from TV Ads](SL-Tasks.ipynb#Advertisement-&-Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "The linear regression model is simple and effective in a wide range of circumstances where one variable is proportional to another.\n",
    "\n",
    "If there is a more complex relationship, we need a more flexible model:\n",
    "\n",
    "$$ y = \\alpha *x + \\beta x^2 + \\gamma x^3 + \\ldots \\zeta x^N $$\n",
    "\n",
    "We can augment the dataset with additional attributes constructed by rising the attribute’s values to different exponents, and then fitting a multivariate linear model to these new attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task - [Following on from predicting Sales from TV Ads](SL-Tasks.ipynb#Non-Linear-Regression-(Ads/Sales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Regression\n",
    "\n",
    "![NLR](img/nonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task - [model sin(x)](SL-Tasks.ipynb#Non-Linear-Regression-(x*sin(x)))\n",
    "\n",
    "```\n",
    "# Function to be modelled\n",
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 10, 30).reshape(-1, 1) \n",
    "\n",
    "# generate points and keep a subset of them for training\n",
    "x = np.linspace(0, 10, 30).reshape(-1, 1) \n",
    "rng = np.random.RandomState(3)\n",
    "rng.shuffle(x)\n",
    "x = np.sort(x[:20])\n",
    "y = f(x)\n",
    "```\n",
    "\n",
    "Experiment with modelling the function using Polynomial features of different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task - [model f(x)=(20*x + rand)](SL-Tasks.ipynb#Non-Linear-Regression-(x*sin(x)))\n",
    "\n",
    "```\n",
    "# Define the functions\n",
    "\n",
    "def f(x):\n",
    "  return 20*x\n",
    "\n",
    "def fnoisy(x):\n",
    "  curr = 20*x+np.random.normal(0, 20,len(x)).reshape(-1, 1) \n",
    "  return curr\n",
    "```\n",
    "\n",
    "Experiment with modelling the function using Polynomial features of different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overfitting vs Noise\n",
    "\n",
    "![noise](img/noise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
