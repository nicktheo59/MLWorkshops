{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101 in Python\n",
    "### Led by Matteo Hessel, UCL ML Alumni, currently a Research Engineer at Google DeepMind, and Bojan Vujatovic, also UCL ML Alumni\n",
    "\n",
    "[Video recording available here](https://youtu.be/GAusrpSVVPw?list=PLBYyZ2a9bVqyN34JPCzEinA_cXT2ckrCa)\n",
    "\n",
    "1. Clone [this repository](https://github.com/bojanvujatovic/unsupervised-learning-workshop)\n",
    "```\n",
    "git clone https://github.com/bojanvujatovic/unsupervised-learning-workshop.git\n",
    "cd unsupervised-learning-workshop    \n",
    "```\n",
    "\n",
    "2. Check if you have pip installed\n",
    "```\n",
    "pip --version  \n",
    "```\n",
    "If not, [download](https://bootstrap.pypa.io/get-pip.py) and run with python:\n",
    "```\n",
    "curl https://bootstrap.pypa.io/get-pip.py   (for Mac/Linux)\n",
    "python get-pip.py \n",
    "``` \n",
    "\n",
    "3. Install all the dependecies necessary for the project (stored in [`requirements.txt`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/requirements.txt)) on your machine in the cloned repo (if it fails  `pip install --upgrade pip`)\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Experience is provided as a stream of multidimensional data which may come in various forms: textual, visual, audio, etc…\n",
    "\n",
    "**The goal is to find in the data some hidden structure, that can help us explain, compress or recode the data itself.**\n",
    "\n",
    "### Focus on:\n",
    "- Clustering (Explain, e.g. through a “generative” story)\n",
    "- Feature Extraction (Recode, e.g. creating new attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "We group the data samples in such a way that elements in the same group are more similar to each other than to those in other groups.\n",
    "\n",
    "The key element is: what does *similar* mean? How do we *measure* similarity?\n",
    "\n",
    "- Clusters may focus on some external property (they look similar)\n",
    "- Clusters may also identify some latent factor, such as all elements being generated by some common sub-process (a “generative story”)\n",
    "\n",
    "### K-Means\n",
    "\n",
    "#### Idea\n",
    "Partition all data in k clusters, identified by a center. Clusters selected to minimize the sum of squared distances between each point and the center of the corresponding cluster.\n",
    "\n",
    "$$ \\operatorname{arg\\,min}_s \\sum\\limits_{i=1}^k \\sum\\limits_{x \\in S_i} \\|x - \\mu_i\\|^2 $$\n",
    "\n",
    "In order to minimize the sum of squared distances each data point is assigned to the cluster whose center is nearest.\n",
    "How do we find the ideal locations of the centers?\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Randomly pick $K$ initial cluster centers.\n",
    "\n",
    "**do**:\n",
    "\n",
    "       Assign each element to the cluster that has the closest center.\n",
    "       Recompute the centers as baricenter of the elements of the cluster: \n",
    "$$ m_i^{new} = \\frac{1}{N_i} \\sum\\limits_{n \\in N_i} x^n $$\n",
    "\n",
    "**while** (centers are different from previous iteration)\n",
    "\n",
    "#### How can we meaningfully select K?\n",
    "![image](img/meaningful.png)\n",
    "\n",
    "#### What happens with non-spherical clusters?\n",
    "![image](img/nonsphere.png)\n",
    "Already with supervised learning I stressed that there is no unique solution to these kind of problems. There is no one algorithm you can always use. With clustering this may even more true.\n",
    "\n",
    "Finding good clusters is about understanding your data, using the results of your algorithms to guide your search.\n",
    "\n",
    "If you find that everytime you run your algorithm you get totally different results, that might be suggesting that you are not effectivelt capturing the underlying structure. \n",
    "\n",
    "Plotting and visualizing the boundaries of your clusters also may help.\n",
    "\n",
    "\n",
    "#### What happens with heterogeneous densities?\n",
    "![image](img/heterogeneous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Practical K-Means 1: Blob Clustering\n",
    "- artificially creating blobs (clusters) of different type\n",
    "- running K-Means to try to discover them\n",
    "- [check out and experiment](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/blobs_kmeans.py)\n",
    "```\n",
    "python blobs_kmeans.py\n",
    "```\n",
    "![image](img/blob.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Practical K-Means 2: Colour Quantisation\n",
    "\n",
    "- complete the script [`colour_quatisation_exercise.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/colour_quatisation_exercise.py). (if you get stuck, there is [`colour_quatisation_solutions.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/colour_quatisation_solutions.py))\n",
    "\n",
    "#### Fit KMeans with sample from image colours (faster)\n",
    "\n",
    "- run clustering on the colours (pixels) of the image\n",
    "- replace each pixel with its centroid\n",
    "- try different values of k, what do you notice?\n",
    "```\n",
    "python color_quatisation.py\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "### K-Means: Other Limitations\n",
    "\n",
    "- Does not manage naturally discrete variables\n",
    "- No principled way of dealing with missing variables\n",
    "\n",
    "Sensitive to outliers:\n",
    "\n",
    "- All data points are assigned to some cluster\n",
    "- All data points contribute to compute the new centers\n",
    "\n",
    "Run multiple experiments:\n",
    "\n",
    "- Algorithm may converge to local optima\n",
    "- Result depends on initialization\n",
    "\n",
    "Normalize attributes:\n",
    "\n",
    "- Not scale invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Spectral Clustering\n",
    "\n",
    "#### Idea\n",
    "Spectral clustering provides one way to deal with non spherical clusters. The idea is based on a probabilistic model analogous to Page Rank.\n",
    "\n",
    "A cluster is a group of closely *connected* samples of arbitrary shape\n",
    "\n",
    "- Connectivity is defined as a function of the distance between samples\n",
    "- Connectivity is propagated so that far distant samples connected by a sequence of tightly connected samples result closely connected\n",
    "\n",
    "This is a classic dataset that is very difficult to cluster meaningfully using k means:\n",
    "![img](img/spectral1.png)\n",
    "\n",
    "The vectors for points on the same connected part of the data will tend to become identical. The more you look far in the future, and will all have very high values for points in the same connected part, and very low probabilities for those in the other connected component. So in this case you will get basically two groups of vectors, with vectors within each group being very similar and in a almost spherical way. This then becomes the ideal setting for applying k-means.\n",
    "![img](img/spectral2.png)\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "Define the similarity matrix A as function of the distances $ A_{i,j} = e^{- \\lambda \\| x^i - x^j \\|^2} $\n",
    "\n",
    "Define the transition matrix P as a function on similarities $ p(i|j) = \\frac{A_{i,j}}{\\sum\\nolimits_{i} A{i,j}} $\n",
    "\n",
    "For each data point $x$ :\n",
    "\n",
    "      compute an approximation of the stationary distribution: \n",
    "$$ \\phi_k(x) = P^k one-hot (x) $$\n",
    "\n",
    "The vectors $ \\phi_k(x) $ will arrange in very tight semi-spherical clusters\n",
    "Identify these clusters using k-means\n",
    "\n",
    "![img](img/power.png)\n",
    "\n",
    "This is a general and powerful idea. If the problem is difficult, it might be because of how you look at it.\n",
    "If you take your data and you transform it suitably, Then the problem might become quite easy.\n",
    "\n",
    "In this case we take datapoints which are aranged in a weird shape and we map each point to a new vector in such a way that, regardless of the shape of the clusters, data points which belong to tightly connected groups will be mapped to tight and approximately spherical clusters the kind of problem we now very well how to solve. Just using k-means!\n",
    "\n",
    "The same idea may apply in countless problems, not only clustering.\n",
    "An entire subfield of unsupervised learning, called feature extraction, focuses exactly on this finding representations of your data that make it easier to solve other problems.\n",
    "\n",
    "Among the problems that can benefit from this there are all supervised learning tasks.\n",
    "The most common approach to any SL task will indeed usually include a preprocessing step in which feature extraction algorithm are used to transform the data into a new representation where classification or regression are easier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Extraction\n",
    "\n",
    "Feature Extraction is the subfield of unsupervised learning that deals with building better representations of your data.\n",
    "\n",
    "A feature extraction algorithm takes as input a set of vectors, and returns a mapping transforming the original data points to new vectors.\n",
    "\n",
    "$$ x = (x_1, x_1, \\ldots , x_1) \\in R^n  $$\n",
    "$$Extract : D \\rightarrow \\phi(x)$$\n",
    "$$ dim(\\phi(x)) < dim(x) $$\n",
    "\n",
    "#### Vectors are important here!\n",
    "A vector is an ordered sequence of elements. \n",
    "\n",
    "Each location in the sequence measures how much the vectors extends along one specific direction.\n",
    "The set of directions, each associated with a location in your vector forms what is called a Reference System.\n",
    "\n",
    "![img](img/vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "### Idea\n",
    "PCA  =  [*Principal Component Analysis*](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "PCA selects a new coordinate system of smaller dimensionality\n",
    "\n",
    "The new attributes of the resulting vectors are linear combinations of the original attributes\n",
    "\n",
    "\n",
    "The new reference system is selected in order to preserve as much information as possible: minimize reconstruction error. \n",
    "This is done maximizing the variability of data along each direction. \n",
    "It also provides an ordering of the resulting directions. \n",
    "We can then remove the directions that are less important in the sense that the different datapoints differ very little in their values.\n",
    "\n",
    "#### Example 1\n",
    "![img](img/eg1.png)\n",
    "\n",
    "#### Example 2\n",
    "![img](img/eg2.png)\n",
    "\n",
    "\n",
    "### Reconstruction Error\n",
    "PCA gives you an explicit way of computing a new representation. It also provides a way of reconstructin the orginal data from the new representation of reduced dimensionality.\n",
    "\n",
    "\n",
    "The more aggressively you reduce the representation’s dimensionality the more information is lost, leading to poorer reconstruction. This gives us a nice way of selecting how many directions to use.\n",
    "\n",
    "![img](img/reconstruction.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Practical PCA 1: MNIST: PCA + kNN\n",
    "\n",
    "- PCA helps supervised tasks\n",
    "- Reduced overfitting\n",
    "- 100 optimal numDimensions\n",
    "- Check out and experiment: [`python mnist_pca_knn.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/mnist_pca_knn.py)\n",
    "\n",
    "![img](img/KNN.png)\n",
    "\n",
    "--- \n",
    "\n",
    "### Practical PCA 2: Modify the above\n",
    "\n",
    "- Modify the experiment\n",
    "- Improve performance\n",
    "- Combine PCA with better SL algorithms (e.g. Decision Trees, Random Forests)\n",
    "\n",
    "---\n",
    "\n",
    "### Practical PCA 3: Reconstruct\n",
    "\n",
    "MNIST digits dataset, reducing dimensionality with PCA: [`python mnist_pca_visualise.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/mnist_pca_visualise.py)\n",
    "\n",
    "![img](img/reconstruction.png)\n",
    "\n",
    "- Many correlated features (pixels) in images\n",
    "- Helps with compression \n",
    "\n",
    "---\n",
    "\n",
    "### Practical PCA 4: Eigenfaces\n",
    "\n",
    "- complete the script [`eigenfaces_exercise.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/eigenfaces_exercise.py). (if you get stuck, there is [`eigenfaces_solution.py`](https://github.com/bojanvujatovic/unsupervised-learning-workshop/blob/master/eigenfaces_solutions.py))\n",
    "- apply PCA to face images\n",
    "- reshape principal components and plot as images\n",
    "- can you explain what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
